{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score, recall_score\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading, Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('application_data_min.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning for features that are used in dataset\n",
    "df['CNT_FAM_MEMBERS'].describe()\n",
    "df.loc[df['CNT_FAM_MEMBERS'].isnull()] = 0\n",
    "df = df.dropna(subset = ['AMT_ANNUITY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08073196986006459\n",
      "0    282674\n",
      "1     24825\n",
      "Name: TARGET, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TARGET = our predicted value, 1 = default, 0 = no default\n",
    "y = df['TARGET']\n",
    "print(y.mean())\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our classification model, we chose the following features:\n",
    "\n",
    "- AMT_INCOME_TOTAL\n",
    "- AMT_CREDIT\n",
    "- AMT_ANNUITY\n",
    "- CODE_GENDER\n",
    "- CNT_FAM_MEMBERS\n",
    "- NAME_CONTRACT_TYPE\n",
    "- NAME_EDUCATION_TYPE\n",
    "\n",
    "We wanted to limit the number of features that the user would have to input in our application. In addition, we wanted to choose features that we believed could be relevant in predicting card defaults/delayed card payments, as well as features that could be easily remembered by users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns: Count_fam_members, income, credit, annuity, contract type, gender, education\n",
    "X = df.drop(columns = ['TARGET'])\n",
    "X = pd.get_dummies(X, drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression, Gradient Boosted Trees?, Decision Tree Classifier, KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classification_models = [LogisticRegression(), KNeighborsClassifier(), DecisionTreeClassifier(), RandomForestClassifier()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification w/ Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we attempted to fit an SGD Classifier to the dataset. We also fit a logistic regression model to our data. However, because only ~8% of the data defaulted, we saw an obvious class imbalance. This led to our model predicting very few defaults (if at all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model Name  Train Accuracy  Test Accuracy  Precision  Recall  \\\n",
       "0      LogisticRegression            0.92           0.92       0.00    0.00   \n",
       "1    KNeighborsClassifier            0.92           0.91       0.15    0.02   \n",
       "3  RandomForestClassifier            0.98           0.91       0.15    0.03   \n",
       "2  DecisionTreeClassifier            0.98           0.86       0.12    0.11   \n",
       "\n",
       "   F1 score  \n",
       "0      0.00  \n",
       "1      0.03  \n",
       "3      0.05  \n",
       "2      0.11  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLA_columns = []\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "row_index = 0\n",
    "\n",
    "for alg in classification_models:\n",
    "    predicted = alg.fit(X_train, y_train).predict(X_test)\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index,'Model Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'Train Accuracy'] = round(alg.score(X_train, y_train), 2)\n",
    "    MLA_compare.loc[row_index, 'Test Accuracy'] = round(alg.score(X_test, y_test), 2)\n",
    "    MLA_compare.loc[row_index, 'Precision'] = round(precision_score(y_test, predicted),2)\n",
    "    MLA_compare.loc[row_index, 'Recall'] = round(recall_score(y_test, predicted),2)\n",
    "    MLA_compare.loc[row_index, 'F1 score'] = round(f1_score(y_test, predicted),2)\n",
    "    row_index+=1\n",
    "    MLA_compare.sort_values(by = ['Test Accuracy'], ascending = False, inplace = True)  \n",
    "    \n",
    "MLA_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, best one is KNeighborsClassifier - close train and test accuracy so probably no overfitting, precision and recall not equal to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier()\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "train_pred = KNN.predict(X_train)\n",
    "test_pred = KNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[211169,    836],\n",
       "       [ 17318,   1301]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70030,   639],\n",
       "       [ 6094,   112]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91274797, 0.9123252 , 0.91343089, 0.9122439 , 0.91273029])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(KNN, X, y, cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification w/ Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Oversampling: Counter({0: 212005, 1: 212005})\n"
     ]
    }
   ],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy='minority', random_state = 42)\n",
    "X_train_o, y_train_o = oversample.fit_resample(X_train, y_train)\n",
    "print('After Oversampling:', Counter(y_train_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model Name  Train Accuracy  Test Accuracy  Precision  Recall  \\\n",
       "3  RandomForestClassifier            0.96           0.85       0.12    0.14   \n",
       "2  DecisionTreeClassifier            0.96           0.82       0.11    0.18   \n",
       "1    KNeighborsClassifier            0.90           0.73       0.10    0.31   \n",
       "0      LogisticRegression            0.51           0.49       0.08    0.53   \n",
       "\n",
       "   F1 score  \n",
       "3      0.13  \n",
       "2      0.14  \n",
       "1      0.16  \n",
       "0      0.14  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLA_columns = []\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "row_index = 0\n",
    "\n",
    "for alg in classification_models:\n",
    "    predicted = alg.fit(X_train_o, y_train_o).predict(X_test)\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index,'Model Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'Train Accuracy'] = round(alg.score(X_train_o, y_train_o), 2)\n",
    "    MLA_compare.loc[row_index, 'Test Accuracy'] = round(alg.score(X_test, y_test), 2)\n",
    "    MLA_compare.loc[row_index, 'Precision'] = round(precision_score(y_test, predicted),2)\n",
    "    MLA_compare.loc[row_index, 'Recall'] = round(recall_score(y_test, predicted),2)\n",
    "    MLA_compare.loc[row_index, 'F1 score'] = round(f1_score(y_test, predicted),2)\n",
    "    row_index+=1\n",
    "    MLA_compare.sort_values(by = ['Test Accuracy'], ascending = False, inplace = True)  \n",
    "    \n",
    "MLA_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we think that DecisionTreeClassifier is the best - while test accuracy is lower, it has a higher recall and f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9629772882715031"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_o = DecisionTreeClassifier().fit(X_train_o, y_train_o)\n",
    "predict_train = dt_o.predict(X_train_o)\n",
    "\n",
    "dt_o.score(X_train_o, y_train_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[199628,   3321],\n",
       "       [ 12377, 208684]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(predict_train, y_train_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8184975609756098"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test = dt_o.predict(X_test)\n",
    "dt_o.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[61810,  5094],\n",
       "       [ 8859,  1112]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(predict_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86343089, 0.8640813 , 0.86452033, 0.8636748 , 0.86393275])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(dt_o, X, y, cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy='minority', random_state = 42)\n",
    "X_train_s, y_train_s = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model Name  Train Accuracy  Test Accuracy  Precision  Recall  \\\n",
       "3  RandomForestClassifier            0.98           0.89       0.13    0.07   \n",
       "2  DecisionTreeClassifier            0.98           0.84       0.10    0.12   \n",
       "1    KNeighborsClassifier            0.88           0.75       0.10    0.26   \n",
       "0      LogisticRegression            0.53           0.58       0.09    0.46   \n",
       "\n",
       "   F1 score  \n",
       "3      0.09  \n",
       "2      0.11  \n",
       "1      0.14  \n",
       "0      0.15  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLA_columns = []\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "row_index = 0\n",
    "\n",
    "for alg in classification_models:\n",
    "    predicted = alg.fit(X_train_s, y_train_s).predict(X_test)\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index,'Model Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'Train Accuracy'] = round(alg.score(X_train_s, y_train_s), 2)\n",
    "    MLA_compare.loc[row_index, 'Test Accuracy'] = round(alg.score(X_test, y_test), 2)\n",
    "    MLA_compare.loc[row_index, 'Precision'] = round(precision_score(y_test, predicted),2)\n",
    "    MLA_compare.loc[row_index, 'Recall'] = round(recall_score(y_test, predicted),2)\n",
    "    MLA_compare.loc[row_index, 'F1 score'] = round(f1_score(y_test, predicted),2)\n",
    "    row_index+=1\n",
    "    MLA_compare.sort_values(by = ['Test Accuracy'], ascending = False, inplace = True)  \n",
    "    \n",
    "MLA_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree classifier is the best? - second highest test accuracy, but better in terms of recall and f1 score than RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.979590104006981"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_s = DecisionTreeClassifier().fit(X_train_s, y_train_s)\n",
    "predict_train = dt_s.predict(X_train_s)\n",
    "\n",
    "dt_s.score(X_train_s, y_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[208908,   5557],\n",
       "       [  3097, 206448]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(predict_train, y_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8443967479674797"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test = dt_s.predict(X_test)\n",
    "dt_s.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70030,   639],\n",
       "       [ 6094,   112]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86386992, 0.86463415, 0.86460163, 0.86304065, 0.86485959])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(dt_s, X, y, cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classfication with Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: results are going to change drastically once they're sampled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_train = X_y_train = pd.concat([X_train.reset_index(drop = True),\n",
    "                       y_train.reset_index(drop = True)], axis = 1)\n",
    "\n",
    "# Create data sets for defaults and non-defaults\n",
    "nondefaults = X_y_train[X_y_train['TARGET'] == 0]\n",
    "defaults = X_y_train[X_y_train['TARGET'] == 1]\n",
    "\n",
    "# Undersample the non-defaults\n",
    "nondefaults_under = nondefaults.sample(len(defaults), random_state = 42)\n",
    "\n",
    "# Concatenate the undersampled nondefaults with defaults\n",
    "X_y_train_undersampled = pd.concat([nondefaults_under.reset_index(drop = True),\n",
    "                             defaults.reset_index(drop = True)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_u = X_y_train_undersampled.drop(columns = ['TARGET'])\n",
    "y_train_u = X_y_train_undersampled['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model Name  Train Accuracy  Test Accuracy  Precision  Recall  \\\n",
       "3  RandomForestClassifier            0.69           0.56       0.10    0.56   \n",
       "1    KNeighborsClassifier            0.65           0.55       0.10    0.55   \n",
       "2  DecisionTreeClassifier            0.66           0.55       0.09    0.53   \n",
       "0      LogisticRegression            0.52           0.49       0.08    0.53   \n",
       "\n",
       "   F1 score  \n",
       "3      0.17  \n",
       "1      0.16  \n",
       "2      0.16  \n",
       "0      0.14  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLA_columns = []\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "row_index = 0\n",
    "\n",
    "for alg in classification_models:\n",
    "    predicted = alg.fit(X_train_u, y_train_u).predict(X_test)\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index,'Model Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'Train Accuracy'] = round(alg.score(X_train_s, y_train_s), 2)\n",
    "    MLA_compare.loc[row_index, 'Test Accuracy'] = round(alg.score(X_test, y_test), 2)\n",
    "    MLA_compare.loc[row_index, 'Precision'] = round(precision_score(y_test, predicted),2)\n",
    "    MLA_compare.loc[row_index, 'Recall'] = round(recall_score(y_test, predicted),2)\n",
    "    MLA_compare.loc[row_index, 'F1 score'] = round(f1_score(y_test, predicted),2)\n",
    "    row_index+=1\n",
    "    MLA_compare.sort_values(by = ['Test Accuracy'], ascending = False, inplace = True)  \n",
    "    \n",
    "MLA_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this one, RandomForestClassifier is the best - best test accuracy, precision, recall, and f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_u = RandomForestClassifier()\n",
    "rfc_u.fit(X_train_u, y_train_u)\n",
    "\n",
    "train_pred = rfc_u.predict(X_train_u)\n",
    "train_all_pred = rfc_u.predict(X_train)\n",
    "test_pred = rfc_u.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13129,  5490],\n",
       "       [ 5386, 13233]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train_u, train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[118854,  93151],\n",
       "       [  5386,  13233]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, train_all_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38505, 32164],\n",
       "       [ 2811,  3395]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91274797, 0.9123252 , 0.91343089, 0.9122439 , 0.91273029])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(rfc_u, X, y, cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, best seems to be the Decision Tree classifier with random oversampling - it has a relatively high test accuracy (0.82), recall of 0.1, and precision of 0.17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_o = DecisionTreeClassifier().fit(X_train_o, y_train_o)\n",
    "predict_train = dt_o.predict(X_train_o)\n",
    "\n",
    "dt_o.score(X_train_o, y_train_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.54225177e-01, 1.64881187e-01, 1.70879247e-01, 2.21903022e-01,\n",
       "       4.62213125e-02, 4.23717898e-03, 1.54354952e-03, 1.57100338e-03,\n",
       "       8.51505591e-03, 0.00000000e+00, 3.18458508e-05, 1.22957999e-02,\n",
       "       4.72382566e-03, 2.46527320e-03, 6.50652232e-03])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_o.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
