# Analyzing Credit Card Defaults

(app.gif)

## Abstract
Our goal was to create an application that educates people about how financial institutions can use machine learning and personal data to determine how risky lending money to that person will be. Using data from Kaggle (ADD LINK HERE), we created a simple decision tree model that users can interact with to see how different inputs affect the model prediction. We also provide visualizations of the data for individuals to learn about the features and see patterns in the data. Finally, by better understanding our machine learning model and patterns in our data, we hope that people can take the insights they learned to put themselves in a better position to not default on bills in the future. 

## Project Goals
It’s widely known that financial institutions today are increasingly reliant on technology such as machine learning to help them assess the risk that comes with lending money to people. However, while the use of this technology is growing, the understanding of how this technology works is not. Now, more than ever, people are becoming perplexed about why financial institutions are viewing them a certain way, and what characteristics lead to a high interest rate or a rejected credit card application. 
Thus, the basic goal that we were attempting to accomplish with this project is to educate people about the relationship between their personal information and their financial institution’s view of them as well as how the models that their financial institutions are currently using might work. While we do not know for certain what features the models that are deployed in practice use, we decided to use obvious features that we think all of these models consider to create a simple algorithm that predicts whether or not someone is going to default on their credit card payments. 
We want people to take away a couple main ideas when using our application. The first is that we want people to be able to enter their information to get a baseline understanding of where they might currently stand with these machine learning models. Then, users should be able to manipulate their entry to get a real time understanding of how the model can view the user differently if they slightly alter their inputs. 
This leads to the second main take away, which is that by using and experimenting with our model by using different combinations of inputs, people are going to gain a basic understanding of how these machine learning models might work. To illustrate, we believe that it is important that people understand that not all features are treated equally, and allowing the user to play around with the algorithm’s inputs will lead them to see which features models might place more weight over others. Additionally, we want people to understand that machine learning isn’t magic, and we want to use visualizations to help people understand why our model is predicting a certain way.
Finally, by gaining a surface level understanding of how our model works, we believe that people can possibly get better at managing their credit card bills in the future. To illustrate, if people are worried that they might potentially default on their credit card bills, gaining a surface level understanding of how our model balances certain features can help them understand how to minimize their default risk. For example, users who expect to have a lower income in the future can play around with our algorithm to see what is a potential cutoff amount for the credit amount of the loan before our algorithm predicts that they can default. Using this knowledge, users can then ramp back on their spending habits to ensure that their credit card bill stays below this credit threshold to minimize their default risk. 

## Design
We wanted our application to flow like a story, and like a story, we started out with basic details such as what a default means and what our model predicted for our users input. We hoped that the user would spend some experimenting with our model to gain a basic understanding of how machine learning models work, so we made sure that our model prediction updated each time the user changed the inputs on the sidebar.
After this section, we decided to create a section where users are able to view univariate visualizations of our dataset. For these visualizations, we sampled 5000 rows of the original dataset (with the same proportion of defaults as the original dataset) to avoid a maximum rows error with Altair. An interesting design decision that we made was to create 2 side-by-side visualizations for each variable, one for default data and one for non-default data, allowing users to compare and contrast these two plots. We believe this will help immensely towards achieving our goal of having our users understand that machine learning models aren’t magic and there is a clear reason as to why the model is behaving the way it does. Additionally, we created a pink line in these charts to give the users a visual indication of where their input falls within these graphs. 
Good interfaces are consistent, so for our multivariate exploration section, we decided to visualize the relationship between a variable to two other variables with a strong relationship. In this section, we really wanted the user to be able to see that the variables that we are considering are not always independent. By seeing and understanding the relationship between multiple variables, users are then able to form more accurate and deeper insights about the data. We wanted to facilitate this further by adding in the brush interaction for our first multivariate visualization, and the selection interaction for our second multivariate visualization. We decided to use the brush interaction as it is effective and linking the two visualizations together. Because it allows users to be able to look through the same window for both visualizations, they are then able to see more clearly how the relationship between the two variables in each visualization differ. As for the selection interaction, we thought that it was the best interaction that allows users to be able to toggle between default and non-default data in our multivariate analysis section. 
Finally, one of our goals was to have people gain a better understanding of machine learning models, which includes a glimpse at how they work and also knowing that they are never perfect. To accomplish the first goal, we felt that the most impactful visualization would be showing them the weight that our model put on each feature. We wanted to include the feature weight graph after the visualizations as we hoped that after looking at the data and seeing the patterns in it, our users are able to understand why our model is putting more weight on certain features. Next, to show that the model we created was not a perfect fit for the data, we visualized a confusion matrix in the form of a heatmap. The confusion matrix shows the predicted vs. the actual values for the test set, and can be used to calculate the accuracy, precision, and recall of our model. From the confusion matrix, it is clear that the model had error in predicting, as there were values where the predicted and actual values were different.
We considered several alternatives before arriving at the final version of our side. The first alternative we considered was splitting our app up into multiple pages. We thought that this would make the app look less intimidating and efficient as we hoped that loading small chunks of our app would be better than waiting a couple of seconds when initially accessing our app. We decided against the pages approach to our application layout because it did not increase our app’s load times, and it turned out to provide a worse overall experience. To illustrate, there is still a long load time each time a user enters a new page, which proves to be really frustrating if the user wants to jump back and forth between pages. Additionally, to improve our model’s accuracy, we initially decided to use about 50 features. While this would have provided a more accurate model, we decided against it for multiple reasons. First, forcing a user to enter 50 columns is way too many, leading to the fact that the user is most likely not going to interact with our model. Additionally, performance is a large part of a positive user experience, and creating visualizations with 50 columns versus 7 would have made our app frustratingly slow. We decided to limit the features in our dataset to the following columns: 

- CODE_GENDER: Gender of the client
- AMT_INCOME_TOTAL: Total income of the client
- AMT_CREDIT: Credit amount of the loan
- AMT_ANNUITY: Loan annuity
- NAME_EDUCATION_TYPE: Level of highest education the client achieved
- NAME_CONTRACT_TYPE: Identification if loan is cash or revolving

We chose to only use these columns to (1) avoid overfitting the model, and to (2) limit the inputs required for the user. These columns were explicitly chosen because they are easy for a user to understand, and because we believed they were important features for the predicted default model. 

## Development
Initially, we had to decide on the focus of the project and find a relevant dataset to use. We ended up choosing this dataset on Kaggle because the columns were well-defined (many columns for other datasets were the results of PCA) and because the dataset had enough rows to create a meaningful prediction. This was done through messaging and through virtually meeting with each other. It took around an hour to complete both of these steps. 

Next, we used supervised learning to build a classification model. We decided to fit a Decision Tree classifier with random oversampling to the data. We chose this method because it yielded the best results - overall, it had a higher test accuracy, precision, and recall than the K-Nearest Neighbors and Logistic Regression models. We chose to use random oversampling to fix the class imbalance issue. Because there were few defaults, our initial models predicted each input as a non-default. We chose to oversample the rows where TARGET = 1 so that the model would predict defaults as well as non-defaults. Since Juliette had prior knowledge in ML, she primarily did this part. It took approximately 3 hours to clean, preprocess, and fit the data. A more comprehensive explanation of the modeling process can be found in the Jupyter notebook “Application Data Classification.ipynb”

For the web app portion of the project, we first laid out the app skeleton of what our app was going to look like. We laid out all the static content, such as the page headers, descriptions, and controls on the sidebar. Then, we added basic test graphs to the application to see how the graphs added to the user experience of our application and whether we needed to edit the skeleton of our app to accommodate for more detailed, complex graphs in the future. We initially were going to split our visualizations into multiple pages, but we found that doing so did not help with performance and did not improve the web app appearance. Nathan was primarily in charge of this part, and he spent 3 hours working on the streamlit content (not including visualizations).

Finally, we worked together to create visualizations for the project. This was equally split between the two members of the team, and we each spent at least 15 hours constructing the different visualizations. Admittedly, this was the hardest part of the report, as neither of us worked with Altair before this assignment. We decided early on the types of visualizations we wanted to construct (univariate EDA, interactive multivariate linked charts, and visualizations about the model). The hardest part was figuring out how to convert our thoughts and initial ideas into correct Altair syntax. Even with the example gallery as inspiration, we found ourselves having to debug many errors. 
